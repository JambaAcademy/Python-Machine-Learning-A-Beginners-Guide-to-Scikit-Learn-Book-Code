{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1604b80b",
   "metadata": {},
   "source": [
    "### 8.2 BAGGING (BOOTSTRAP AGGREGATING) \n",
    "\n",
    "Bagging stands for Bootstrap Aggregating; it is a technique that creates multiple versions of the original dataset by randomly sampling the data with replacement. Each sample is then used to train a separate model, and the final prediction is made by averaging the predictions of all the models. Bagging is particularly useful for reducing the variance of the final model.\n",
    "\n",
    "The basic idea behind bagging is to create multiple subsets of the original dataset by randomly sampling the data with replacement. Each subset is used to train a separate model, and the final prediction is made by averaging the predictions of all the models. This technique can be used with any type of model, but it is particularly useful for decision tree models, which are known to have high variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9bfb98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 87.50%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate random data\n",
    "np.random.seed(42)\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_classes=2)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit a Random Forest classifier using bagging\n",
    "clf = RandomForestClassifier(n_estimators=10, max_features='sqrt')\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "score = clf.score(X_test, y_test)\n",
    "print(\"Accuracy: %.2f%%\" % (score * 100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25aabd43",
   "metadata": {},
   "source": [
    "### 8.3 BOOSTING: ADAPTING THE WEAK TO THE STRONG \n",
    "\n",
    "Boosting is a technique that combines multiple weak models to create a stronger model. The basic idea behind boosting is to train a series of models sequentially, where each model tries to correct the mistakes made by the previous model. Boosting can be used with any type of model, but it is particularly useful for decision tree models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d44c3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 85.50%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generate random data\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=42)\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit AdaBoost classifier with decision tree as base estimator\n",
    "dt_clf = DecisionTreeClassifier(max_depth=1)\n",
    "ada_clf = AdaBoostClassifier(base_estimator=dt_clf, n_estimators=50, learning_rate=0.1, random_state=42)\n",
    "ada_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict using trained AdaBoost classifier\n",
    "y_pred = ada_clf.predict(X_test)\n",
    "\n",
    "# Calculate accuracy score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy score: {:.2f}%\".format(accuracy*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8b612b",
   "metadata": {},
   "source": [
    "#### Gradient Boosting\n",
    "\n",
    "Gradient Boosting is a powerful ensemble learning method used in supervised learning problems for classification and regression. It combines the power of decision trees with the concept of gradient descent, and its flexibility and high accuracy make it a popular choice for many machine learning problems.\n",
    "\n",
    "Gradient Boosting works by iteratively training a sequence of decision trees. In each iteration, a new decision tree is trained on the residual errors of the previous tree. The predictions of each tree are then combined to give the final prediction.\n",
    "\n",
    "One of the key advantages of Gradient Boosting is that it can handle a variety of loss functions, which makes it a versatile method for different types of machine learning problems. The most commonly used loss functions are the mean squared error (MSE) for regression problems and the log loss for classification problems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99b674bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error:  1600.1626279309244\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Generate random dataset\n",
    "np.random.seed(42)\n",
    "X, y = make_regression(n_samples=1000, n_features=10, noise=20, random_state=42)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Fit Gradient Boosting model\n",
    "gb = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "gb.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test data\n",
    "y_pred = gb.predict(X_test)\n",
    "\n",
    "# Evaluate model performance\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean squared error: \", mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e74f85",
   "metadata": {},
   "source": [
    "#### XGBoost (Extreme Gradient Boosting) \n",
    "\n",
    "XGBoost (Extreme Gradient Boosting) is a popular implementation of gradient boosting. It is known for its speed and accuracy in handling large-scale datasets. In this article, we will provide a coding example for XGBoost with random dataset and explain how it works.\n",
    "\n",
    "XGBoost is a machine learning algorithm that uses decision trees for regression and classification problems. The algorithm works by building a series of trees, where each tree corrects the mistakes of the previous tree. The trees are built using a greedy algorithm that finds the best split at each node.\n",
    "\n",
    "XGBoost uses a technique called gradient boosting to optimize the trees. Gradient boosting involves adding new trees to the model that predict the residual errors of the previous trees. The idea is to gradually improve the model by reducing the errors at each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd2b4593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 40.00%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generate random data\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 5)\n",
    "y = np.random.randint(0, 2, 100)\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create XGBoost DMatrix objects\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# Set hyperparameters for XGBoost\n",
    "params = {\n",
    "    'max_depth': 3, \n",
    "    'eta': 0.1, \n",
    "    'objective': 'binary:logistic', \n",
    "    'eval_metric': 'error'\n",
    "}\n",
    "\n",
    "# Train the model\n",
    "num_round = 50\n",
    "xg_model = xgb.train(params, dtrain, num_round)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = xg_model.predict(dtest)\n",
    "y_pred = [1 if x > 0.5 else 0 for x in y_pred]\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "578a8a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting xgboost\n",
      "  Downloading xgboost-1.7.4-py3-none-win_amd64.whl (89.1 MB)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from xgboost) (1.21.5)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from xgboost) (1.7.3)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-1.7.4\n"
     ]
    }
   ],
   "source": [
    "### If you need to install xgboost you can use the below command. Just uncomment the below command and run it, it will install the xgboost library on your system.\n",
    "#####!pip install xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eeff451",
   "metadata": {},
   "source": [
    "#### LightGBM (Light Gradient Boosting Machine)\n",
    "\n",
    "LightGBM (Light Gradient Boosting Machine) is a gradient boosting framework that uses tree-based learning algorithms. It is designed to be lightweight, fast, and scalable, making it a popular choice for large-scale machine learning tasks.\n",
    "\n",
    "LightGBM is a high-performance gradient boosting framework that uses decision trees as its base model. It is designed to handle large datasets with millions of instances and features. LightGBM uses a technique called \"leaf-wise growth\" to grow decision trees, which allows it to find the optimal split points more efficiently.\n",
    "\n",
    "One of the key features of LightGBM is its ability to handle categorical features. Unlike other gradient boosting frameworks, LightGBM can directly handle categorical features without the need for one-hot encoding. This can significantly reduce the memory footprint and training time for datasets with a large number of categorical features.\n",
    "\n",
    "Another important feature of LightGBM is its ability to handle imbalanced datasets. It provides a parameter called \"is_unbalance\" that can be set to true to automatically adjust the weights of the training instances based on their class distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5297a192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting lightgbm\n",
      "  Downloading lightgbm-3.3.5-py3-none-win_amd64.whl (1.0 MB)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from lightgbm) (1.7.3)\n",
      "Requirement already satisfied: wheel in c:\\programdata\\anaconda3\\lib\\site-packages (from lightgbm) (0.37.1)\n",
      "Requirement already satisfied: scikit-learn!=0.22.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from lightgbm) (1.0.2)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from lightgbm) (1.21.5)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn!=0.22.0->lightgbm) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn!=0.22.0->lightgbm) (2.2.0)\n",
      "Installing collected packages: lightgbm\n",
      "Successfully installed lightgbm-3.3.5\n"
     ]
    }
   ],
   "source": [
    "### If you need to install xgboost you can use the below command\n",
    "####!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "458b0290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 405, number of negative: 395\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000401 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1275\n",
      "[LightGBM] [Info] Number of data points in the train set: 800, number of used features: 5\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.506250 -> initscore=0.025001\n",
      "[LightGBM] [Info] Start training from score 0.025001\n",
      "[1]\tvalid_0's binary_logloss: 0.691565\n",
      "[2]\tvalid_0's binary_logloss: 0.692726\n",
      "[3]\tvalid_0's binary_logloss: 0.692952\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's binary_logloss: 0.692721\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's binary_logloss: 0.695557\n",
      "[6]\tvalid_0's binary_logloss: 0.696472\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's binary_logloss: 0.696717\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's binary_logloss: 0.698908\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's binary_logloss: 0.700466\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\tvalid_0's binary_logloss: 0.700591\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's binary_logloss: 0.704054\n",
      "[12]\tvalid_0's binary_logloss: 0.705939\n",
      "[13]\tvalid_0's binary_logloss: 0.705556\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[14]\tvalid_0's binary_logloss: 0.706172\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[15]\tvalid_0's binary_logloss: 0.706068\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[16]\tvalid_0's binary_logloss: 0.707179\n",
      "[17]\tvalid_0's binary_logloss: 0.70882\n",
      "[18]\tvalid_0's binary_logloss: 0.71083\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[19]\tvalid_0's binary_logloss: 0.711684\n",
      "[20]\tvalid_0's binary_logloss: 0.712718\n",
      "[21]\tvalid_0's binary_logloss: 0.714972\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[22]\tvalid_0's binary_logloss: 0.716036\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[23]\tvalid_0's binary_logloss: 0.718041\n",
      "[24]\tvalid_0's binary_logloss: 0.720613\n",
      "[25]\tvalid_0's binary_logloss: 0.722842\n",
      "[26]\tvalid_0's binary_logloss: 0.724153\n",
      "[27]\tvalid_0's binary_logloss: 0.725682\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[28]\tvalid_0's binary_logloss: 0.726811\n",
      "[29]\tvalid_0's binary_logloss: 0.727935\n",
      "[30]\tvalid_0's binary_logloss: 0.730487\n",
      "[31]\tvalid_0's binary_logloss: 0.731579\n",
      "[32]\tvalid_0's binary_logloss: 0.734573\n",
      "[33]\tvalid_0's binary_logloss: 0.736659\n",
      "[34]\tvalid_0's binary_logloss: 0.738086\n",
      "[35]\tvalid_0's binary_logloss: 0.741115\n",
      "[36]\tvalid_0's binary_logloss: 0.743113\n",
      "[37]\tvalid_0's binary_logloss: 0.744252\n",
      "[38]\tvalid_0's binary_logloss: 0.744788\n",
      "[39]\tvalid_0's binary_logloss: 0.747726\n",
      "[40]\tvalid_0's binary_logloss: 0.747726\n",
      "[41]\tvalid_0's binary_logloss: 0.750435\n",
      "[42]\tvalid_0's binary_logloss: 0.751581\n",
      "[43]\tvalid_0's binary_logloss: 0.755408\n",
      "[44]\tvalid_0's binary_logloss: 0.756469\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[45]\tvalid_0's binary_logloss: 0.756231\n",
      "[46]\tvalid_0's binary_logloss: 0.757122\n",
      "[47]\tvalid_0's binary_logloss: 0.758707\n",
      "[48]\tvalid_0's binary_logloss: 0.760788\n",
      "[49]\tvalid_0's binary_logloss: 0.762721\n",
      "[50]\tvalid_0's binary_logloss: 0.765255\n",
      "[51]\tvalid_0's binary_logloss: 0.767652\n",
      "[52]\tvalid_0's binary_logloss: 0.769482\n",
      "[53]\tvalid_0's binary_logloss: 0.769466\n",
      "[54]\tvalid_0's binary_logloss: 0.769851\n",
      "[55]\tvalid_0's binary_logloss: 0.772584\n",
      "[56]\tvalid_0's binary_logloss: 0.774316\n",
      "[57]\tvalid_0's binary_logloss: 0.774726\n",
      "[58]\tvalid_0's binary_logloss: 0.775193\n",
      "[59]\tvalid_0's binary_logloss: 0.774064\n",
      "[60]\tvalid_0's binary_logloss: 0.77568\n",
      "[61]\tvalid_0's binary_logloss: 0.777917\n",
      "[62]\tvalid_0's binary_logloss: 0.777175\n",
      "[63]\tvalid_0's binary_logloss: 0.778555\n",
      "[64]\tvalid_0's binary_logloss: 0.779642\n",
      "[65]\tvalid_0's binary_logloss: 0.779528\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[66]\tvalid_0's binary_logloss: 0.780618\n",
      "[67]\tvalid_0's binary_logloss: 0.781026\n",
      "[68]\tvalid_0's binary_logloss: 0.781023\n",
      "[69]\tvalid_0's binary_logloss: 0.781746\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[70]\tvalid_0's binary_logloss: 0.78234\n",
      "[71]\tvalid_0's binary_logloss: 0.782397\n",
      "[72]\tvalid_0's binary_logloss: 0.784479\n",
      "[73]\tvalid_0's binary_logloss: 0.786402\n",
      "[74]\tvalid_0's binary_logloss: 0.78695\n",
      "[75]\tvalid_0's binary_logloss: 0.787374\n",
      "[76]\tvalid_0's binary_logloss: 0.789778\n",
      "[77]\tvalid_0's binary_logloss: 0.791225\n",
      "[78]\tvalid_0's binary_logloss: 0.792375\n",
      "[79]\tvalid_0's binary_logloss: 0.792847\n",
      "[80]\tvalid_0's binary_logloss: 0.794199\n",
      "[81]\tvalid_0's binary_logloss: 0.793752\n",
      "[82]\tvalid_0's binary_logloss: 0.794808\n",
      "[83]\tvalid_0's binary_logloss: 0.795584\n",
      "[84]\tvalid_0's binary_logloss: 0.796673\n",
      "[85]\tvalid_0's binary_logloss: 0.798619\n",
      "[86]\tvalid_0's binary_logloss: 0.801164\n",
      "[87]\tvalid_0's binary_logloss: 0.802726\n",
      "[88]\tvalid_0's binary_logloss: 0.80221\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[89]\tvalid_0's binary_logloss: 0.802586\n",
      "[90]\tvalid_0's binary_logloss: 0.803255\n",
      "[91]\tvalid_0's binary_logloss: 0.804329\n",
      "[92]\tvalid_0's binary_logloss: 0.806218\n",
      "[93]\tvalid_0's binary_logloss: 0.808654\n",
      "[94]\tvalid_0's binary_logloss: 0.809022\n",
      "[95]\tvalid_0's binary_logloss: 0.810519\n",
      "[96]\tvalid_0's binary_logloss: 0.810895\n",
      "[97]\tvalid_0's binary_logloss: 0.81292\n",
      "[98]\tvalid_0's binary_logloss: 0.814783\n",
      "[99]\tvalid_0's binary_logloss: 0.817224\n",
      "[100]\tvalid_0's binary_logloss: 0.817874\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate random data\n",
    "X = np.random.rand(1000, 5)\n",
    "y = np.random.randint(0, 2, 1000)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train_data = lgb.Dataset(X[:800], label=y[:800])\n",
    "test_data = lgb.Dataset(X[800:], label=y[800:])\n",
    "\n",
    "# Set parameters\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9\n",
    "}\n",
    "\n",
    "# Train model\n",
    "model = lgb.train(params, train_data, valid_sets=[test_data])\n",
    "\n",
    "# Make predictions on test set\n",
    "y_pred = model.predict(X[800:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7946a152",
   "metadata": {},
   "source": [
    "### 8.4 STACKING: BUILDING A POWERFUL META MODEL \n",
    "\n",
    "Stacking is a technique that combines multiple models to create a stronger model. It works by training a series of models using different subsets of the data and then using the predictions of these models as inputs to train a final model. Stacking can be used with any type of model, but it is particularly useful for combining models of different types.\n",
    "\n",
    "The basic idea behind stacking is to divide the data into two subsets: the training set and the holdout set. The training set is used to train multiple models, and the holdout set is used to make predictions for these models. The predictions of the models are then concatenated with the original features and used to train a final model, called the meta-model. The final model can be any type of model such as a linear model, decision tree, or neural network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "952abcda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking AUC score: 0.9643\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "\n",
    "# Generate random dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=42)\n",
    "\n",
    "# Define base models\n",
    "model_1 = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "model_2 = GradientBoostingClassifier(n_estimators=50, random_state=42)\n",
    "\n",
    "# Define meta model\n",
    "meta_model = LogisticRegression(random_state=42)\n",
    "\n",
    "# Create k-fold cross-validation splits\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Train base models and make predictions on test set\n",
    "X_meta_train = np.zeros((len(X), 2))\n",
    "for i, model in enumerate([model_1, model_2]):\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        model.fit(X[train_index], y[train_index])\n",
    "        X_meta_train[test_index, i] = model.predict_proba(X[test_index])[:, 1]\n",
    "\n",
    "# Train meta model on meta features and evaluate\n",
    "score = cross_val_score(meta_model, X_meta_train, y, cv=skf, scoring='roc_auc').mean()\n",
    "print(f\"Stacking AUC score: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae9f726",
   "metadata": {},
   "source": [
    "### 8.5 BLENDING \n",
    "\n",
    "Blending is a technique that is similar to stacking, but it combines the predictions of multiple models rather than the models themselves. It works by training multiple models on different subsets of the data, then using the predictions of these models to train a final model. The main difference between blending and stacking is that blending uses the predictions of the models as inputs to the final model, while stacking uses the models themselves as inputs.\n",
    "\n",
    "In scikit-learn, blending can be implemented by training multiple models on different subsets of the data, then using the predictions of these models as inputs to train a final model. Here's an example of how to use blending to train a model on a dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a2eb8b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.5098260853286533\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Generate random dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=5, noise=0.5)\n",
    "\n",
    "# Split the data into two parts\n",
    "split = 0.8\n",
    "split_idx = int(split * len(X))\n",
    "X_train = X[:split_idx]\n",
    "y_train = y[:split_idx]\n",
    "X_blend = X[split_idx:]\n",
    "y_blend = y[split_idx:]\n",
    "\n",
    "# Train base models\n",
    "models = [LinearRegression(), DecisionTreeRegressor()]\n",
    "for model in models:\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the blend data using base models\n",
    "blend_preds = np.column_stack([model.predict(X_blend) for model in models])\n",
    "\n",
    "# Train blending model on the blend data\n",
    "blend_model = LinearRegression()\n",
    "blend_model.fit(blend_preds, y_blend)\n",
    "\n",
    "# Make predictions on the test data using the blended model\n",
    "test_preds = np.column_stack([model.predict(X[split_idx:]) for model in models])\n",
    "final_preds = blend_model.predict(test_preds)\n",
    "\n",
    "# Calculate the RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y[split_idx:], final_preds))\n",
    "print(f\"RMSE: {rmse}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94df6ae",
   "metadata": {},
   "source": [
    "### 8.6 ROTATION FOREST \n",
    "\n",
    "Rotation Forest is an ensemble learning method that was introduced by Rodriguez et al. in 2006. It belongs to the family of decision tree-based ensemble methods, and its main idea is to increase the diversity among the base classifiers by applying a random feature transformation before building each tree.\n",
    "\n",
    "Rotation Forest uses a technique called PCA (Principal Component Analysis) to randomly select a subset of features from the original dataset, and then rotates these features in a way that maximizes the variance of the transformed features. This process is repeated for each tree, resulting in a set of diverse base classifiers that are less correlated with each other.\n",
    "\n",
    "The idea behind Rotation Forest is that by applying random feature transformations, it is more likely to capture the underlying structure of the data and reduce the risk of overfitting. Additionally, the use of PCA ensures that the transformed features are uncorrelated and therefore, more informative.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "13b70fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy: 53.00%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Generate random data\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 10)\n",
    "y = np.random.randint(2, size=100)\n",
    "\n",
    "# Define the number of trees and features to select\n",
    "num_trees = 10\n",
    "num_features = 3\n",
    "\n",
    "# Initialize an empty set of rotated datasets\n",
    "rotated_datasets = []\n",
    "\n",
    "# For each tree in the forest\n",
    "for i in range(num_trees):\n",
    "    # Randomly select k features from the dataset\n",
    "    selected_features = np.random.choice(X.shape[1], size=num_features, replace=False)\n",
    "    # Compute the PCA projection matrix for the selected features\n",
    "    pca = PCA(n_components=num_features)\n",
    "    pca.fit(X[:, selected_features])\n",
    "    projection_matrix = pca.components_\n",
    "    # Rotate the dataset using the projection matrix\n",
    "    rotated_data = np.dot(X[:, selected_features], projection_matrix.T)\n",
    "    rotated_datasets.append(rotated_data)\n",
    "\n",
    "# Train a meta-model using the rotated datasets\n",
    "meta_features = np.hstack(rotated_datasets)\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(meta_features, y)\n",
    "\n",
    "# Evaluate the performance using 10-fold cross-validation\n",
    "kf = KFold(n_splits=10)\n",
    "scores = []\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    # Compute the rotated datasets for the training and test sets\n",
    "    rotated_train = []\n",
    "    rotated_test = []\n",
    "    for dataset in rotated_datasets:\n",
    "        rotated_train.append(dataset[train_index])\n",
    "        rotated_test.append(dataset[test_index])\n",
    "    meta_train = np.hstack(rotated_train)\n",
    "    meta_test = np.hstack(rotated_test)\n",
    "    # Train a meta-model on the rotated training set\n",
    "    model = DecisionTreeClassifier()\n",
    "    model.fit(meta_train, y_train)\n",
    "    # Evaluate the meta-model on the rotated test set\n",
    "    score = model.score(meta_test, y_test)\n",
    "    scores.append(score)\n",
    "\n",
    "print(\"Mean accuracy: {:.2f}%\".format(np.mean(scores) * 100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9c4624",
   "metadata": {},
   "source": [
    "### 8.7 CASCADING CLASSIFIERS \n",
    "\n",
    "Cascading classifiers is a type of ensemble learning method that combines multiple weak classifiers into a single strong classifier. This method is often used in object detection applications where the objective is to identify an object within an image or a video.\n",
    "\n",
    "The basic idea of cascading classifiers is to break down the object detection problem into multiple stages or layers. Each layer is responsible for detecting a particular aspect of the object. For example, the first layer might detect the edges of the object, the second layer might detect its shape, and the final layer might identify the object itself.\n",
    "\n",
    "The advantage of cascading classifiers is that it allows for faster and more efficient object detection. Since each layer is designed to detect a specific feature of the object, it can quickly eliminate any parts of the image that do not contain that feature. This reduces the number of false positives and speeds up the overall detection process.\n",
    "\n",
    "To implement cascading classifiers, we can use a combination of feature extraction techniques and machine learning algorithms. For feature extraction, we might use techniques such as Haar cascades, which are commonly used in object detection applications. For machine learning algorithms, we might use techniques such as support vector machines (SVMs) or neural networks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6d3eb4c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.20\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generate random dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, random_state=42)\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Define cascading classifiers pipeline\n",
    "cascading_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svm', SVC(kernel='linear', probability=True))\n",
    "#     ('random_forest', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# Train the first classifier on the entire training set\n",
    "cascading_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set using the first classifier\n",
    "first_classifier_predictions = cascading_pipeline.predict(X_test)\n",
    "\n",
    "# Extract the samples which were misclassified by the first classifier\n",
    "misclassified_samples_mask = first_classifier_predictions != y_test\n",
    "misclassified_samples_X = X_test[misclassified_samples_mask]\n",
    "misclassified_samples_y = y_test[misclassified_samples_mask]\n",
    "\n",
    "# Train the second classifier on the misclassified samples\n",
    "cascading_pipeline.fit(misclassified_samples_X, misclassified_samples_y)\n",
    "\n",
    "# Make predictions on the test set using both classifiers\n",
    "final_predictions = cascading_pipeline.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the final predictions\n",
    "accuracy = accuracy_score(y_test, final_predictions)\n",
    "\n",
    "# Print the accuracy score\n",
    "print(f'Accuracy score: {accuracy:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b14ea14",
   "metadata": {},
   "source": [
    "### 8.8 ADVERSARIAL TRAINING \n",
    "\n",
    "Adversarial examples are crafted by adding a small perturbation to the input data that is not noticeable to the human eye but can significantly change the model's output. Adversarial training involves generating such examples and training the model with them, which makes the model more robust and able to handle adversarial attacks.\n",
    "\n",
    "One common approach to generate adversarial examples is the Fast Gradient Sign Method (FGSM). This method computes the gradient of the loss function with respect to the input data and adds a small perturbation in the direction that maximizes the loss. The perturbation is scaled by a small value, which controls the magnitude of the perturbation.\n",
    "\n",
    "Adversarial training involves generating such adversarial examples and incorporating them into the training data. During training, the model learns to recognize and classify these examples correctly, which improves its ability to handle adversarial attacks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8b9aea69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting cleverhans\n",
      "  Downloading cleverhans-4.0.0-py3-none-any.whl (92 kB)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from cleverhans) (1.16.0)\n",
      "Collecting tensorflow-probability\n",
      "  Downloading tensorflow_probability-0.19.0-py2.py3-none-any.whl (6.7 MB)\n",
      "Requirement already satisfied: matplotlib in c:\\programdata\\anaconda3\\lib\\site-packages (from cleverhans) (3.5.1)\n",
      "Collecting easydict\n",
      "  Downloading easydict-1.10.tar.gz (6.4 kB)\n",
      "Collecting mnist\n",
      "  Downloading mnist-0.2.2-py2.py3-none-any.whl (3.5 kB)\n",
      "Requirement already satisfied: nose in c:\\programdata\\anaconda3\\lib\\site-packages (from cleverhans) (1.3.7)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from cleverhans) (1.1.0)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from cleverhans) (1.7.3)\n",
      "Requirement already satisfied: pycodestyle in c:\\programdata\\anaconda3\\lib\\site-packages (from cleverhans) (2.7.0)\n",
      "Collecting absl-py\n",
      "  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from cleverhans) (1.21.5)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->cleverhans) (3.0.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->cleverhans) (1.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->cleverhans) (2.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->cleverhans) (21.3)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->cleverhans) (4.25.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->cleverhans) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->cleverhans) (9.0.1)\n",
      "Collecting gast>=0.3.2\n",
      "  Downloading gast-0.5.3-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: cloudpickle>=1.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-probability->cleverhans) (2.0.0)\n",
      "Requirement already satisfied: decorator in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-probability->cleverhans) (5.1.1)\n",
      "Collecting dm-tree\n",
      "  Downloading dm_tree-0.1.8-cp39-cp39-win_amd64.whl (101 kB)\n",
      "Building wheels for collected packages: easydict\n",
      "  Building wheel for easydict (setup.py): started\n",
      "  Building wheel for easydict (setup.py): finished with status 'done'\n",
      "  Created wheel for easydict: filename=easydict-1.10-py3-none-any.whl size=6506 sha256=4d50c11736700e791344b3eb6e94f9471643a149c5d0cc48237c143ecb1cc25f\n",
      "  Stored in directory: c:\\users\\rajen\\appdata\\local\\pip\\cache\\wheels\\0d\\9a\\a9\\02f3a5f0c6b2c57184661770360c58db8166f5c877780e98f2\n",
      "Successfully built easydict\n",
      "Installing collected packages: gast, dm-tree, absl-py, tensorflow-probability, mnist, easydict, cleverhans\n",
      "Successfully installed absl-py-1.4.0 cleverhans-4.0.0 dm-tree-0.1.8 easydict-1.10 gast-0.5.3 mnist-0.2.2 tensorflow-probability-0.19.0\n"
     ]
    }
   ],
   "source": [
    "###If cleverhans is not installed on your device, you need to install cleverhans by using the below command:\n",
    "### !pip install cleverhans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "31b9c3a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  ERROR: Error [WinError 2] The system cannot find the file specified while executing command git clone -q http://github.com/tensorflow/cleverhans.git 'C:\\Users\\rajen\\Documents\\KnowledgeBase\\machine learning\\src\\cleverhans'\n",
      "ERROR: Cannot find command 'git' - do you have 'git' installed and in your PATH?\n"
     ]
    }
   ],
   "source": [
    "### !pip install -qq -e git+http://github.com/tensorflow/cleverhans.git#egg=cleverhans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "61a75f56",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cleverhans.future'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [32]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msvm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SVC\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcleverhans\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfuture\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mattacks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fast_gradient_method\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcleverhans\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfuture\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mattacks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m projected_gradient_descent\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcleverhans\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfuture\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mattacks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sparse_l1_descent_attack\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cleverhans.future'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from cleverhans.future.tf2.attacks import fast_gradient_method\n",
    "from cleverhans.future.tf2.attacks import projected_gradient_descent\n",
    "from cleverhans.future.tf2.attacks import sparse_l1_descent_attack\n",
    "from cleverhans.future.tf2.attacks import carlini_wagner_l2_attack\n",
    "\n",
    "# Generate random data\n",
    "np.random.seed(42)\n",
    "X_train = np.random.rand(100, 2)\n",
    "y_train = (X_train[:, 0] < X_train[:, 1]).astype(int)\n",
    "\n",
    "# Create SVM classifier\n",
    "clf = SVC(kernel='linear', probability=True)\n",
    "\n",
    "# Train SVM classifier on original data\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_train)\n",
    "print('Accuracy on original data:', accuracy_score(y_train, y_pred))\n",
    "\n",
    "# Generate adversarial examples using FGSM method\n",
    "eps = 0.1\n",
    "X_adv = fast_gradient_method(clf, X_train, eps=eps, norm=np.inf, targeted=False)\n",
    "\n",
    "# Train SVM classifier on adversarial examples\n",
    "clf.fit(X_adv, y_train)\n",
    "y_pred_adv = clf.predict(X_train)\n",
    "print('Accuracy on adversarial data:', accuracy_score(y_train, y_pred_adv))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3528a40a",
   "metadata": {},
   "source": [
    "### 8.9 VOTING CLASSIFIER \n",
    "\n",
    "Ensemble learning methods combine multiple machine learning models to improve the predictive performance of the overall model. One of the simplest and most popular ensemble methods is the Voting Classifier, which combines the predictions of multiple individual classifiers to make a final prediction. In this section, we will discuss the concept of the Voting Classifier and provide a real-life coding example.\n",
    "\n",
    "#### What is a Voting Classifier? \n",
    "\n",
    "A Voting Classifier is an ensemble learning method that combines the predictions of multiple individual classifiers to make a final prediction. The idea behind the Voting Classifier is that by combining the predictions of multiple classifiers, the overall prediction will be more accurate and less prone to errors than any individual classifier.\n",
    "\n",
    "The Voting Classifier can be implemented in two ways: hard voting and soft voting. In hard voting, each individual classifier makes a binary prediction, and the final prediction is based on the majority vote of the individual predictions. In soft voting, each individual classifier produces a probability estimate for each class, and the final prediction is based on the average probability of each class across all individual classifiers.\n",
    "#### Coding Example: \n",
    "\n",
    "Let's implement a Voting Classifier on a random dataset using the scikit-learn library. We will first generate a random dataset using the make_classification function of scikit-learn, which generates a random n-class classification problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "483da801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.865\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generate a random dataset\n",
    "X, y = make_classification(n_samples=1000, n_classes=2, random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the individual classifiers\n",
    "clf1 = DecisionTreeClassifier(random_state=42)\n",
    "clf2 = LogisticRegression(random_state=42)\n",
    "clf3 = SVC(kernel='linear', probability=True, random_state=42)\n",
    "\n",
    "# Define the Voting Classifier\n",
    "voting_clf = VotingClassifier(estimators=[('dt', clf1), ('lr', clf2), ('svm', clf3)], voting='hard')\n",
    "\n",
    "# Train the Voting Classifier\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = voting_clf.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy of the Voting Classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy:', accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b927e3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
